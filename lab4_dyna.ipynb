{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnkbtelPTuHuJhTAPlCDHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsaldana/reinforcement-learning-course/blob/main/lab4_dyna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: TD and Dyna\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3O73YRqcSpVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Implement SARSA with n-step TD (n=5) on CliffWalking\n",
        "\n",
        "**Objective:**  \n",
        "In this exercise, you will implement the **SARSA algorithm** using **n-step temporal-difference learning with n=5**. You will apply your implementation to the **CliffWalking environment** in Gymnasium, and analyze how multi-step returns influence learning compared to standard 1-step SARSA.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `CliffWalking-v1`\n",
        "\n",
        "---\n",
        "\n",
        "### Instructions\n",
        "1. Implement **SARSA with n-step TD updates (n=5)**:\n",
        "   - Maintain an action-value table \\(Q(s,a)\\).\n",
        "   - Use ε-greedy exploration.\n",
        "   - Store states, actions, and rewards for the last 5 steps.\n",
        "   - After each step, compute the n-step return: G_t\n",
        "   - Update \\(Q(s_t,a_t)\\) toward \\(G_t\\).\n",
        "\n",
        "2. Train your agent for several thousand episodes (e.g., 5,000).\n",
        "\n",
        "3. Plot the **episode rewards over time** to visualize learning progress.\n",
        "\n",
        "4. Compare qualitatively with 1-step SARSA:\n",
        "   - Does n-step SARSA converge faster or slower?\n",
        "   - How do the policies differ near the cliff?\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- Python code implementing SARSA with TD(5) (notebook in Github).  \n",
        "- A plot of episode number vs episode return (plot in a cell below).  \n",
        "- A short discussion (1 paragraph) comparing the results with standard SARSA.  \n"
      ],
      "metadata": {
        "id": "mnJD-ntoxjeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORsNHBnkSbyS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Starter code for Exercise (you can use this code, or extend your code from previous lab)\n",
        "Implement SARSA with TD(5) on CliffWalking-v1\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Environment\n",
        "env = gym.make(\"CliffWalking-v1\")\n",
        "\n",
        "# Parameters\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "alpha = 0.1           # step size (learning rate)\n",
        "gamma = 0.99          # discount factor\n",
        "epsilon = 0.1         # epsilon for epsilon-greedy policy\n",
        "n_step = 5            # number of steps for TD(n)\n",
        "n_episodes = 5000\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "def epsilon_greedy(state):\n",
        "    \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "# Track returns\n",
        "episode_returns = []\n",
        "\n",
        "for ep in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    action = epsilon_greedy(state)\n",
        "\n",
        "    # Buffers to store the trajectory\n",
        "    states = deque()\n",
        "    actions = deque()\n",
        "    rewards = deque()\n",
        "\n",
        "    T = float(\"inf\")\n",
        "    t = 0\n",
        "    G = 0\n",
        "    done = False\n",
        "\n",
        "    while True:\n",
        "        if t < T:\n",
        "            # Take real step in the environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                T = t + 1\n",
        "            else:\n",
        "                next_action = epsilon_greedy(next_state)\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "\n",
        "        # Time index for state/action to update\n",
        "        tau = t - n_step + 1\n",
        "        if tau >= 0:\n",
        "            # TODO: Compute the n-step return G for state tau\n",
        "            # Hint: use rewards[tau : tau+n] plus Q(s_t+n, a_t+n) if not terminal\n",
        "\n",
        "            # Example structure:\n",
        "            G = 0.0\n",
        "            # accumulate discounted rewards\n",
        "            for i in range(tau, min(tau + n_step, T)):\n",
        "                G += (gamma ** (i - tau)) * rewards[i]\n",
        "            if tau + n_step < T:\n",
        "                s_tau_n = states[tau + n_step]\n",
        "                a_tau_n = actions[tau + n_step]\n",
        "                G += (gamma ** n_step) * Q[s_tau_n, a_tau_n]\n",
        "\n",
        "            # TODO: Update Q[states[tau], actions[tau]] toward G\n",
        "            Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
        "\n",
        "        if tau == T - 1:\n",
        "            break\n",
        "\n",
        "        t += 1\n",
        "\n",
        "    episode_returns.append(sum(rewards))\n",
        "\n",
        "# Plot learning curve\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.title(\"SARSA with TD(5) on CliffWalking\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Dyna-Q for CliffWalking\n",
        "\n",
        "**Objective**  \n",
        "Implement **Dyna-Q** on **CliffWalking-v1** and compare its learning performance to **SARSA (1-step)** and **SARSA TD(5)**. You will analyze sample efficiency, stability near the cliff, and sensitivity to planning steps.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `CliffWalking-v1`\n",
        "---\n",
        "\n",
        "### Part A — Dyna-Q (Implementation)\n",
        "1. **Q-table**: maintain `Q[s, a]` (tabular).\n",
        "2. **Model**: learn an empirical model from experience.\n",
        "   - For each observed transition `(s, a, r, s')`, update a dictionary:\n",
        "     - Minimal: store the most recent `(s', r)` for `(s, a)`, **or**\n",
        "     - Advanced: store a **multiset** of outcomes for `(s, a)` with counts (to sample stochastically).\n",
        "3. **Real update (Q-learning)** after each env step:\n",
        "   Q(s,a) ← Q(s,a) + α * (r + γ * max_a' Q(s',a') - Q(s,a))\n",
        "4. **Planning updates**: after each real step, perform `N` simulated updates:\n",
        "   - Sample a previously seen `(s_p, a_p)` from the model.\n",
        "   - Sample `(r_p, s'_p)` from that entry.\n",
        "   - Apply the same Q-learning backup using `(s_p, a_p, r_p, s'_p)`.\n",
        "5. Use epsilon-greedy exploration.\n",
        "\n",
        "---\n",
        "\n",
        "### Part B — Baselines (Re-use / Implement)\n",
        "- **SARSA (1-step)** with ε-greedy:\n",
        "  \\[\n",
        "  Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[r + \\gamma Q(s',a') - Q(s,a)\\big]\n",
        "  \\]\n",
        "- **SARSA TD(5)** (n-step SARSA with \\(n=5\\)), as in Exercise 1.\n",
        "\n",
        "Use the **same** γ, α, ε schedule, and number of episodes for a fair comparison.\n",
        "\n",
        "---\n",
        "\n",
        "### Part C — Experiments & Comparisons\n",
        "1. **Learning curves**: plot **episode index vs. episode return** for:\n",
        "   - Dyna-Q with \\(N \\in \\{5, 20, 50\\}\\)\n",
        "   - SARSA (1-step)\n",
        "   - SARSA TD(5)\n",
        "2. **Sample efficiency**: report the **episode number** at which the average return over a sliding window (e.g., 100 episodes) first exceeds a chosen threshold (e.g., −30).\n",
        "3. **Stability near the cliff**: qualitatively inspect trajectories/policies; does the method hug the cliff or leave a safer margin?\n",
        "4. **Sensitivity to planning steps**: compare Dyna-Q across N; discuss diminishing returns vs. computation.\n",
        "5. **Statistical robustness**: run **≥5 seeds**; plot mean ± std (shaded) or report mean ± std of final returns.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- **Code**: A driver script/notebook that reproduces your plots\n",
        "- **Plots** (embedded in the notebook):\n",
        "  - Learning curves (mean ± std across seeds)\n",
        "  - Optional: heatmap of greedy policy/actions on the grid\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h8NKZuvP5GZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BhimOYaY0zZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Solve FrozenLake with Q-Learning and Dyna-Q (Stochastic Model)\n",
        "\n",
        "**Objective**  \n",
        "Implement and compare **Q-learning** and **Dyna-Q** on Gymnasium’s `FrozenLake-v1`.  \n",
        "For Dyna-Q, your learned **transition model must handle multiple possible next states** per `(s, a)` (stochastic slip), i.e., store and sample **a distribution** over `(s', r)` outcomes rather than a single next state.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `FrozenLake-v1` from `gymnasium.envs.toy_text`.\n",
        "- You can start with map 4×4; and then work with 8×8.\n",
        "- Start → Goal with slippery transitions (stochastic).  \n",
        "- Rewards: `+1` at goal, `0` otherwise (holes terminate with 0).\n",
        "\n",
        "---\n",
        "\n",
        "### Part A — Q-learning (baseline)\n",
        "1. Maintain a tabular action-value function `Q[s, a]`.\n",
        "2. Behavior: ε-greedy over `Q`.\n",
        "3. Update after each real step:\n",
        "   - target = r + γ * max_a' Q[s', a']   (if terminal: target = r)\n",
        "   - Q[s, a] ← Q[s, a] + α * (target − Q[s, a])\n",
        "4. Train for several thousand episodes (e.g., 5,000) with an ε schedule (e.g., 0.2 → 0.01).\n",
        "\n",
        "---\n",
        "\n",
        "### Part B — Dyna-Q with a **stochastic transition model**\n",
        "1. **Empirical model (multinomial):** for each `(s, a)`, maintain a multiset of observed outcomes:\n",
        "   - `model[(s, a)] = [(s'_1, r_1, count_1), (s'_2, r_2, count_2), ...]`\n",
        "   - Update counts whenever you observe `(s, a, r, s')`.\n",
        "2. **Real step update (Q-learning):** same as Part A.\n",
        "3. **Planning steps (N per real step):**\n",
        "   - Sample a previously seen `(s_p, a_p)` uniformly (or with priority).\n",
        "   - Sample `(s'_p, r_p)` **from the empirical distribution** for `(s_p, a_p)` using counts as probabilities.\n",
        "   - Apply the same Q-learning backup with `(s_p, a_p, r_p, s'_p)`.\n",
        "4. Train with the same ε schedule and number of episodes; vary `N ∈ {5, 20, 50}`.\n",
        "\n",
        "---\n",
        "\n",
        "### Experiments & Analysis\n",
        "1. **Learning curves:** plot episode index vs episode return (smoothed) for:\n",
        "   - Q-learning\n",
        "   - Dyna-Q (N=5, 20, 50)\n",
        "2. **Sample efficiency:** report the episode at which the moving-average return (e.g., window 100) first exceeds a threshold (you choose a reasonable value).\n",
        "3. **Effect of stochastic modeling:** briefly explain why storing a distribution over `(s', r)` matters on FrozenLake (slip), and what happens if you store only the most recent outcome.\n",
        "4. **Robustness:** run ≥5 random seeds; report mean ± std of final evaluation returns.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- **Code** for Q-learning and Dyna-Q (with stochastic model).  \n",
        "- **Plots** of learning curves (include legend and axis labels).  \n",
        "- ** Discussion:** why Dyna-Q helps here; impact of N; importance of modeling multiple next states.\n",
        "\n",
        "---\n",
        "\n",
        "### Hints\n",
        "- For terminal transitions (goal/hole), the Q-learning target is simply `target = r` (no bootstrap).  \n",
        "- When sampling from the model, use probabilities `p_i = count_i / sum_j count_j`.  \n",
        "- Tie-break greedy action selection uniformly among argmax actions to avoid bias.  \n",
        "- Keep evaluation **greedy (ε=0)** and consistent across methods (same seeds and episode counts).\n"
      ],
      "metadata": {
        "id": "E4iLQFaGzJLj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7FHlfk700lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}